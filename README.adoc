= LLM Prompts Eval
A test app to figure out the best system prompts for various LLM models, using OpenAI API compatible endpoints.

It is a pure HTML/JS application that runs in the browser, no backend server required.

== API Endpoints

https://platform.openai.com/docs/api-reference/chat/create[OpenAI API]

```ts 

 const OPENAI_API_BASE_URL = 'https://api.openai.com';
 const OPENAI_API_BASE_URL = 'http://localhost:11434'; // for local testing with a proxy server
 const OPENAI_API_BASE_URL = 'http://192.168.29:11434';
```

== Deploy

=== On Firebase Hosting

 cp index.html public/index.html
 cp style.css public/style.css
 firebase deploy --only hosting:chat-qzip-in

https://chat-qzip-in.web.app/

https://chat.qzip.in/

Note: update the firebase.json file with the correct public directory before deploying.

The firebase.json file should have the following content:

 {
   "hosting": {
     "public": "public",
     "ignore": [
       "firebase.json",
       "**/.*",
       "**/node_modules/**"
     ]
   }
 }

Also note that deployed sites cannot use localhost or local network machines as the API base URL, use the deployed URL instead.

Download and run it locally if you wish to try locally hosted Ollama.

=== On GitHub Pages

 git branch -M main
 git remote add origin https://github.com/innomon/chat-qzip.git  

  git push -u origin main
  git subtree push --prefix public origin gh-pages

https://innomon.github.io/chat-qzip/

== Local Development  

run a local web server to serve the files

 npx http-server 

  open http://localhost:8080

Alternatively use the Live Server extension in VS Code.

